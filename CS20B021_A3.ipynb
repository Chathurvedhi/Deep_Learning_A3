{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - CS20B021 - CS6910"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0]) < MAX_LEN and \\\n",
    "        len(p[1]) < MAX_LEN\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('tel_train.csv')\n",
    "valid_data = pd.read_csv('tel_valid.csv')\n",
    "test_data = pd.read_csv('tel_test.csv')\n",
    "\n",
    "# Prepare data in pairs\n",
    "train_pairs = []\n",
    "for line in train_data.values:\n",
    "    train_pairs.append([line[0], line[1]])\n",
    "    \n",
    "valid_pairs = []\n",
    "for line in valid_data.values:\n",
    "    valid_pairs.append([line[0], line[1]])\n",
    "\n",
    "test_pairs = []\n",
    "for line in test_data.values:\n",
    "    test_pairs.append([line[0], line[1]])\n",
    "\n",
    "# Filter pairs with length > MAX_LEN\n",
    "train_pairs = filterPairs(train_pairs)\n",
    "valid_pairs = filterPairs(valid_pairs)\n",
    "test_pairs = filterPairs(test_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "        self.char2index = {}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_chars = 2  # Count SOS and EOS\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "input_lang = Lang('input')\n",
    "output_lang = Lang('output')\n",
    "\n",
    "for pair in train_pairs:\n",
    "    input_lang.addWord(pair[0])\n",
    "    output_lang.addWord(pair[1])\n",
    "\n",
    "def indexesFromWord(lang, word):\n",
    "    return [lang.char2index[char] for char in word]\n",
    "\n",
    "def tensorFromWord(lang, word):\n",
    "    indexes = indexesFromWord(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
    "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 128, embed_size = 64, num_layers = 1, cell_type = 'gru', dropout = 0.1, bidirectional = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Class Variables\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        # Layers and Cells Initialized with parameters\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        if cell_type == 'gru':\n",
    "            self.cell = nn.GRU(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif cell_type == 'rnn':\n",
    "            self.cell = nn.RNN(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif cell_type == 'lstm':\n",
    "            self.cell = nn.LSTM(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        else:\n",
    "            raise ValueError('Invalid cell type specified')\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the cell\n",
    "        if self.cell_type == 'lstm':\n",
    "            output, (hidden, cell) = self.cell(embedded, (hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.cell(embedded, hidden)\n",
    "\n",
    "        return output, hidden, cell\n",
    "    \n",
    "    def initHidden(self):\n",
    "        # Initialize hidden state with zeros\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size,hidden_size = 128, embed_size = 64, num_layers = 1, cell_type = 'gru', dropout = 0.1, bidirectional = False):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Class Variables\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        # Layers and Cells Initialized with parameters\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        if cell_type == 'gru':\n",
    "            self.cell = nn.GRU(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif cell_type == 'rnn':\n",
    "            self.cell = nn.RNN(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif cell_type == 'lstm':\n",
    "            self.cell = nn.LSTM(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        else:\n",
    "            raise ValueError('Invalid cell type specified')\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the cell\n",
    "        if self.cell_type == 'lstm':\n",
    "            output, (hidden, cell) = self.cell(output, (hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.cell(output, hidden)\n",
    "\n",
    "        # Output with softmax\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Initialize hidden state with zeros\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "    def __init__(self, hidden_size = 128, embed_size = 64, num_layers = 1, cell_type = 'gru', dropout = 0.1, bidirectional = False, optimizer = 'sgd', lr = 0.01):\n",
    "        \n",
    "        # Class Variables\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "\n",
    "        # Initialize Encoder and Decoder\n",
    "        self.encoder = Encoder(input_lang.n_chars, hidden_size, embed_size, num_layers, cell_type, dropout, bidirectional).to(device)\n",
    "        self.decoder = Decoder(output_lang.n_chars, hidden_size, embed_size, num_layers, cell_type, dropout, bidirectional).to(device)\n",
    "\n",
    "        # Initialize Optimizer\n",
    "        if optimizer == 'sgd':\n",
    "            self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr = self.lr)\n",
    "            self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr = self.lr)\n",
    "        elif optimizer == 'adam':\n",
    "            self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr = self.lr)\n",
    "            self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr = self.lr)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer specified')\n",
    "        \n",
    "        # Initialize Criterion\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "    def train(self, input_tensor, target_tensor):\n",
    "\n",
    "        # Initialize Encoder Hidden State\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_cell = self.encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(MAX_LEN, self.encoder.hidden_size, device=device)\n",
    "\n",
    "        # Zero Gradients\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Get input and target length\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        # Initialize Loss\n",
    "        loss = 0\n",
    "\n",
    "        # Forward pass through the encoder\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        # Initialize Decoder Hidden State\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "        # Use Teacher Forcing\n",
    "        use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "        \n",
    "        # Forward pass through the decoder\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "        # Backpropagate Loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Parameters\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        return loss.item() / target_length\n",
    "    \n",
    "    def evaluate(self, word: str):\n",
    "        with torch.no_grad():\n",
    "            input_tensor = tensorFromWord(self.input_lang, word)\n",
    "            input_length = input_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "            encoder_cell = self.encoder.initHidden()\n",
    "\n",
    "            encoder_outputs = torch.zeros(MAX_LEN, self.encoder.config.hidden_size, device=device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei],\n",
    "                                                        encoder_hidden, encoder_cell)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "            decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "            decoded_words = []\n",
    "\n",
    "            for di in range(MAX_LEN):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_words.append('<EOS>')\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_words.append(self.output_lang.index2letter[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            return decoded_words\n",
    "        \n",
    "    def accuracy(self, pairs):\n",
    "        \n",
    "        # Returns the accuracy of the model on the given pairs\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for pair in pairs:\n",
    "            output_words = self.evaluate(pair[0])\n",
    "            output_sentence = ''.join(output_words)\n",
    "            if output_sentence == pair[1]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        return correct / total\n",
    "        \n",
    "    def trainEpoch(self, epochs = 5, Log = False):\n",
    "        \n",
    "        # Initializations\n",
    "        start = time.time()\n",
    "        plot_losses = []\n",
    "        print_loss_total = 0  # Reset every 1000 steps\n",
    "        plot_loss_total = 0  # Reset every 100\n",
    "        prev_train_acc = 0\n",
    "        prev_val_acc = 0\n",
    "\n",
    "        # Train for given number of epochs\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print('Epoch: ', epoch)\n",
    "\n",
    "            # Get Tensor Pairs\n",
    "            training_pairs = [tensorFromPair(pair) for pair in train_pairs]\n",
    "\n",
    "            # Train for one epoch\n",
    "            for i in range(1, len(training_pairs) + 1):\n",
    "                training_pair = training_pairs[i - 1]\n",
    "                input_tensor = training_pair[0]\n",
    "                target_tensor = training_pair[1]\n",
    "\n",
    "                loss = self.train(input_tensor, target_tensor)\n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "\n",
    "                # Print Progress\n",
    "                if i % 1000 == 0:\n",
    "                    temp = len(train_pairs)\n",
    "                    print_loss_avg = print_loss_total / 1000\n",
    "                    if Log:\n",
    "                        wandb.log({\"train_loss\": print_loss_avg})\n",
    "                    print_loss_total = 0\n",
    "                    print('%s (%d %d%%) %.4f' % (timeSince(start, i / len(train_pairs)),\n",
    "                                                i, i /  len(train_pairs) * 100, print_loss_avg))\n",
    "\n",
    "\n",
    "            # Train Accuracy\n",
    "            train_acc = self.accuracy(train_pairs)\n",
    "            #print('Train Accuracy: ', train_acc)\n",
    "\n",
    "            # Validation Accuracy\n",
    "            val_acc = self.accuracy(valid_pairs)\n",
    "            #print('Validation Accuracy: ', val_acc)\n",
    "\n",
    "            # Check to end\n",
    "            if val_acc < prev_val_acc or train_acc < prev_train_acc:\n",
    "                break\n",
    "                \n",
    "            # Update previous accuracy\n",
    "            prev_train_acc = train_acc\n",
    "            prev_val_acc = val_acc\n",
    "\n",
    "            # Log to wandb\n",
    "            if Log:\n",
    "                wandb.log({\n",
    "                            \"train_acc\": train_acc, \n",
    "                            \"val_acc\": val_acc,\n",
    "                            \"epoch\": epoch\n",
    "                            })\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_acc = self.accuracy(test_pairs)\n",
    "        #print('Test Accuracy: ', test_acc)\n",
    "\n",
    "        # Log to wandb\n",
    "        if Log:\n",
    "            wandb.log({\"test_acc\": test_acc})\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Seq2Seq(hidden_size = 256)\n",
    "#model.trainEpoch(epochs = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'hidden_size': {\n",
    "            'values': [128, 256, 512]\n",
    "        },\n",
    "        'embed_size': {\n",
    "            'values': [64, 128, 256]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['gru', 'rnn', 'lstm']\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.1, 0.2, 0.3]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['sgd', 'adam']\n",
    "        },\n",
    "        'lr': {\n",
    "            'values': [0.01, 0.001, 0.0001]\n",
    "        },\n",
    "        'epoch': {\n",
    "            'values': [5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS20B021_A3\")\n",
    "\n",
    "def train_sweep_run():\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    run.name = \"epoch:{}_hid:{}_embed:{}_cell:{}_nlayer:{}_drop:{}_opt:{}_lr:{}\".format(config.epoch, config.hidden_size, config.embed_size, config.cell_type, config.num_layers, config.dropout, config.optimizer, config.lr)\n",
    "\n",
    "    model = Seq2Seq(hidden_size = config.hidden_size, embed_size = config.embed_size, num_layers = config.num_layers, cell_type = config.cell_type, dropout = config.dropout, optimizer = config.optimizer, lr = config.lr)\n",
    "    model.trainEpoch(epochs = config.epoch, Log = True)\n",
    "    run.finish()\n",
    "\n",
    "wandb.agent(sweep_id, train_sweep_run, count = 10, project=\"CS20B021_A3\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
