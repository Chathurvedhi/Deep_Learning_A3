{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - CS20B021 - CS6910"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0]) < MAX_LEN and \\\n",
    "        len(p[1]) < MAX_LEN\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('aksharantar_sampled/tel/tel_train.csv')\n",
    "valid_data = pd.read_csv('aksharantar_sampled/tel/tel_valid.csv')\n",
    "test_data = pd.read_csv('aksharantar_sampled/tel/tel_test.csv')\n",
    "\n",
    "# Prepare data in pairs\n",
    "train_pairs = []\n",
    "for line in train_data.values:\n",
    "    train_pairs.append([line[0], line[1]])\n",
    "    \n",
    "valid_pairs = []\n",
    "for line in valid_data.values:\n",
    "    valid_pairs.append([line[0], line[1]])\n",
    "\n",
    "test_pairs = []\n",
    "for line in test_data.values:\n",
    "    test_pairs.append([line[0], line[1]])\n",
    "\n",
    "# Filter pairs with length > MAX_LEN\n",
    "train_pairs = filterPairs(train_pairs)\n",
    "valid_pairs = filterPairs(valid_pairs)\n",
    "test_pairs = filterPairs(test_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "        self.char2index = {}\n",
    "        self.char2count = {}\n",
    "        self.index2char = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_chars = 2  # Count SOS and EOS\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.char2index:\n",
    "            self.char2index[char] = self.n_chars\n",
    "            self.char2count[char] = 1\n",
    "            self.index2char[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.char2count[char] += 1\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "input_lang = Lang('input')\n",
    "output_lang = Lang('output')\n",
    "\n",
    "for pair in train_pairs:\n",
    "    input_lang.addWord(pair[0])\n",
    "    output_lang.addWord(pair[1])\n",
    "\n",
    "def indexesFromWord(lang, word):\n",
    "    return [lang.char2index[char] for char in word]\n",
    "\n",
    "def tensorFromWord(lang, word):\n",
    "    indexes = indexesFromWord(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
    "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 128, embed_size = 64, num_layers = 1, cell_type = 'gru', dropout = 0.1, bidirectional = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Class Variables\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        # Layers and Cells Initialized with parameters\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        if cell_type == 'gru':\n",
    "            self.cell = nn.GRU(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif cell_type == 'rnn':\n",
    "            self.cell = nn.RNN(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif cell_type == 'lstm':\n",
    "            self.cell = nn.LSTM(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        else:\n",
    "            raise ValueError('Invalid cell type specified')\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the cell\n",
    "        if self.cell_type == 'lstm':\n",
    "            output, (hidden, cell) = self.cell(embedded, (hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.cell(embedded, hidden)\n",
    "\n",
    "        return output, hidden, cell\n",
    "    \n",
    "    def initHidden(self):\n",
    "        # Initialize hidden state with zeros\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size,hidden_size = 128, embed_size = 64, num_layers = 1, cell_type = 'gru', dropout = 0.1, bidirectional = False):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Class Variables\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "        # Layers and Cells Initialized with parameters\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        if cell_type == 'gru':\n",
    "            self.cell = nn.GRU(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif cell_type == 'rnn':\n",
    "            self.cell = nn.RNN(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        elif cell_type == 'lstm':\n",
    "            self.cell = nn.LSTM(input_size = self.embed_size, hidden_size = self.hidden_size, num_layers = self.num_layers, dropout = self.dropout, bidirectional = self.bidirectional)\n",
    "        else:\n",
    "            raise ValueError('Invalid cell type specified')\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the cell\n",
    "        if self.cell_type == 'lstm':\n",
    "            output, (hidden, cell) = self.cell(output, (hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.cell(output, hidden)\n",
    "\n",
    "        # Output with softmax\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Initialize hidden state with zeros\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "    def __init__(self, hidden_size = 128, embed_size = 64, num_layers = 1, cell_type = 'gru', dropout = 0.1, bidirectional = False, optimizer = 'sgd', lr = 0.01):\n",
    "        \n",
    "        # Class Variables\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "\n",
    "        # Initialize Encoder and Decoder\n",
    "        self.encoder = Encoder(input_lang.n_chars, hidden_size, embed_size, num_layers, cell_type, dropout, bidirectional).to(device)\n",
    "        self.decoder = Decoder(output_lang.n_chars, hidden_size, embed_size, num_layers, cell_type, dropout, bidirectional).to(device)\n",
    "\n",
    "        # Initialize Optimizer\n",
    "        if optimizer == 'sgd':\n",
    "            self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr = self.lr)\n",
    "            self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr = self.lr)\n",
    "        elif optimizer == 'adam':\n",
    "            self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr = self.lr)\n",
    "            self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr = self.lr)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer specified')\n",
    "        \n",
    "        # Initialize Criterion\n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "    def train(self, input_tensor, target_tensor):\n",
    "\n",
    "        # Initialize Encoder Hidden State\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_cell = self.encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(MAX_LEN, self.encoder.hidden_size, device=device)\n",
    "\n",
    "        # Zero Gradients\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        # Get input and target length\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        # Initialize Loss\n",
    "        loss = 0\n",
    "\n",
    "        # Forward pass through the encoder\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        # Initialize Decoder Hidden State\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "        # Use Teacher Forcing\n",
    "        use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "        \n",
    "        # Forward pass through the decoder\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "        # Backpropagate Loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Parameters\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        return loss.item() / target_length\n",
    "    \n",
    "    def evaluate(self, word: str):\n",
    "        with torch.no_grad():\n",
    "            input_tensor = tensorFromWord(self.input_lang, word)\n",
    "            input_length = input_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "            encoder_cell = self.encoder.initHidden()\n",
    "\n",
    "            encoder_outputs = torch.zeros(MAX_LEN, self.encoder.config.hidden_size, device=device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei],\n",
    "                                                        encoder_hidden, encoder_cell)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "            decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "            decoded_words = []\n",
    "\n",
    "            for di in range(MAX_LEN):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_words.append('<EOS>')\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_words.append(self.output_lang.index2letter[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            return decoded_words\n",
    "        \n",
    "    def accuracy(self, pairs):\n",
    "        \n",
    "        # Returns the accuracy of the model on the given pairs\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for pair in pairs:\n",
    "            output_words = self.evaluate(pair[0])\n",
    "            output_sentence = ''.join(output_words)\n",
    "            if output_sentence == pair[1]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        return correct / total\n",
    "        \n",
    "    def trainEpoch(self, epochs = 5, Log = False):\n",
    "        \n",
    "        # Initializations\n",
    "        start = time.time()\n",
    "        plot_losses = []\n",
    "        print_loss_total = 0  # Reset every 1000 steps\n",
    "        plot_loss_total = 0  # Reset every 100\n",
    "        prev_train_acc = 0\n",
    "        prev_val_acc = 0\n",
    "\n",
    "        # Train for given number of epochs\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print('Epoch: ', epoch)\n",
    "\n",
    "            # Get Tensor Pairs\n",
    "            training_pairs = [tensorFromPair(pair) for pair in train_pairs]\n",
    "\n",
    "            # Train for one epoch\n",
    "            for i in range(1, len(training_pairs) + 1):\n",
    "                training_pair = training_pairs[i - 1]\n",
    "                input_tensor = training_pair[0]\n",
    "                target_tensor = training_pair[1]\n",
    "\n",
    "                loss = self.train(input_tensor, target_tensor)\n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "\n",
    "                # Print Progress\n",
    "                if i % 1000 == 0:\n",
    "                    temp = len(train_pairs)\n",
    "                    print_loss_avg = print_loss_total / 1000\n",
    "                    print_loss_total = 0\n",
    "                    print('%s (%d %d%%) %.4f' % (timeSince(start, i / len(train_pairs)),\n",
    "                                                i, i /  len(train_pairs) * 100, print_loss_avg))\n",
    "\n",
    "                # Plot progess\n",
    "                if i % 100 == 0:\n",
    "                    plot_loss_avg = plot_loss_total / 100\n",
    "                    plot_losses.append(plot_loss_avg)\n",
    "                    plot_loss_total = 0\n",
    "                    \n",
    "            # Training Loss\n",
    "            train_loss = sum(plot_losses) / len(plot_losses)\n",
    "\n",
    "            # Train Accuracy\n",
    "            train_acc = self.accuracy(train_pairs)\n",
    "            print('Train Accuracy: ', train_acc)\n",
    "\n",
    "            # Validation Accuracy\n",
    "            val_acc = self.accuracy(valid_pairs)\n",
    "\n",
    "            # Check to end\n",
    "            if val_acc < prev_val_acc or train_acc < prev_train_acc:\n",
    "                break\n",
    "                \n",
    "            # Update previous accuracy\n",
    "            prev_train_acc = train_acc\n",
    "            prev_val_acc = val_acc\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_acc = self.accuracy(test_pairs)\n",
    "        print('Test Accuracy: ', test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohan/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0m 20s (- 17m 21s) (1000 1%) 28.4639\n",
      "0m 43s (- 17m 45s) (2000 3%) 27.4905\n",
      "1m 1s (- 16m 27s) (3000 5%) 27.4846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m Seq2Seq(hidden_size \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mtrainEpoch(epochs \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[107], line 155\u001b[0m, in \u001b[0;36mSeq2Seq.trainEpoch\u001b[0;34m(self, epochs, Log)\u001b[0m\n\u001b[1;32m    152\u001b[0m input_tensor \u001b[39m=\u001b[39m training_pair[\u001b[39m0\u001b[39m]\n\u001b[1;32m    153\u001b[0m target_tensor \u001b[39m=\u001b[39m training_pair[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 155\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(input_tensor, target_tensor)\n\u001b[1;32m    156\u001b[0m print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m    157\u001b[0m plot_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[107], line 77\u001b[0m, in \u001b[0;36mSeq2Seq.train\u001b[0;34m(self, input_tensor, target_tensor)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# Backpropagate Loss\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     79\u001b[0m \u001b[39m# Update Parameters\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(hidden_size = 256)\n",
    "model.trainEpoch(epochs = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
